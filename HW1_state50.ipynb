{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import lab_util\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = []\n",
    "n_positive = 0\n",
    "n_disp = 0\n",
    "with open(\"./reviews.csv\") as reader:\n",
    "    csvreader = csv.reader(reader)\n",
    "    next(csvreader)\n",
    "    for id, review, label in csvreader:\n",
    "        label = int(label)\n",
    "        # hacky class balancing\n",
    "        if label == 1:\n",
    "            if n_positive == 2000:\n",
    "                continue\n",
    "            n_positive += 1\n",
    "        if len(data) == 4000:\n",
    "            break\n",
    "        data.append((review, label))\n",
    "\n",
    "        if n_disp > 5:\n",
    "            continue\n",
    "        n_disp += 1\n",
    "        # print(\"review:\", review)\n",
    "        # print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
    "        # print()\n",
    "\n",
    "# print(f\"Read {len(data)} total reviews.\")\n",
    "np.random.shuffle(data)\n",
    "reviews, labels = zip(*data)\n",
    "train_reviews = reviews[:3000]\n",
    "train_labels = labels[:3000]\n",
    "val_reviews = reviews[3000:3500]\n",
    "val_labels = labels[3000:3500]\n",
    "test_reviews = reviews[3500:]\n",
    "test_labels = labels[3500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm model\n",
    "class HMM(object):\n",
    "    def __init__(self, num_states, num_words):\n",
    "        self.num_states = num_states\n",
    "        self.num_words = num_words\n",
    "\n",
    "        self.states = range(num_states)\n",
    "        self.symbols = range(num_words)\n",
    "\n",
    "        # initialize the matrix A with random transition probabilities p(j|i)\n",
    "        # A should be a matrix of size `num_states x num_states`\n",
    "        # with rows that sum to 1\n",
    "        self.A = np.random.random([self.num_states, self.num_states])\n",
    "        self.A = self.A / np.sum(self.A, axis=1).reshape(-1, 1) # your code here\n",
    "\n",
    "        # initialize the matrix B with random emission probabilities p(o|i)\n",
    "        # B should be a matrix of size `num_states x num_words`\n",
    "        # with rows that sum to 1\n",
    "        self.B = np.random.random([self.num_states, self.num_words])\n",
    "        self.B = self.B / np.sum(self.B, axis=1).reshape(-1, 1)\n",
    "        # your code here\n",
    "\n",
    "        # initialize the vector pi with a random starting distribution\n",
    "        # pi should be a vector of size `num_states`\n",
    "        self.pi = np.random.random(self.num_states)\n",
    "        self.pi = self.pi / np.sum(self.pi) \n",
    "        # your code here\n",
    "\n",
    "    def generate(self, n):\n",
    "        \"\"\"randomly sample the HMM to generate a sequence.\n",
    "        \"\"\"\n",
    "        # we'll give you this one\n",
    "\n",
    "        sequence = []\n",
    "        # initialize the first state\n",
    "        state = np.random.choice(self.states, p=self.pi)\n",
    "        for i in range(n):\n",
    "            # get the emission probs for this state\n",
    "            b = self.B[state, :]\n",
    "            # emit a word\n",
    "            word = np.random.choice(self.symbols, p=b)\n",
    "            sequence.append(word)\n",
    "            # get the transition probs for this state\n",
    "            a = self.A[state, :]\n",
    "            # update the state\n",
    "            state = np.random.choice(self.states, p=a)\n",
    "        return sequence\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # run the forward algorithm\n",
    "        # this function should return a `len(obs) x num_states` matrix\n",
    "        # where the (i, j)th entry contains p(obs[:t], hidden_state_t = i)\n",
    "\n",
    "        alpha = np.zeros((len(obs), self.num_states))\n",
    "        # your code here!\n",
    "        # print(self.pi)\n",
    "        alpha[0, :] = self.pi * self.B[:, obs[0]]\n",
    "        # print(alphal[0,:])\n",
    "        for t in range(1, len(obs)):\n",
    "            for j in range(self.num_states):\n",
    "            # alpha[t, j] = np.dot(alpha[t-1, :], self.A[:, j]) * self.B[j, obs[t]]\n",
    "                alpha[t, j] = alpha[t-1].dot(self.A[:, j]) * self.B[j, obs[t]]\n",
    "        alpha[alpha == 0] = sys.float_info.min\n",
    "        alpha = np.log(alpha)\n",
    "\n",
    "        return alpha\n",
    "\n",
    "    def backward(self, obs):\n",
    "        # run the backward algorithm\n",
    "        # this function should return a `len(obs) x num_states` matrix\n",
    "        # where the (i, j)th entry contains p(obs[t+1:] | hidden_state_t = i)\n",
    "\n",
    "        beta = np.zeros((len(obs), self.num_states))\n",
    "        # your code here!\n",
    "        beta[len(obs) - 1, :] = np.ones(self.num_states)\n",
    "        for t in range(len(obs)-2, -1, -1):\n",
    "            for j in range(self.num_states):\n",
    "                beta[t, j] = (beta[t+1, :] * self.B[:, obs[t+1]]).dot(self.A[j, :])\n",
    "\n",
    "        beta[beta == 0] = sys.float_info.min\n",
    "        beta = np.log(beta)\n",
    "\n",
    "        return beta\n",
    "        \n",
    "    def forward_backward(self, obs):\n",
    "        # compute forward--backward scores\n",
    "\n",
    "        # logprob is the total log-probability of the sequence obs \n",
    "        # (marginalizing over hidden states)\n",
    "\n",
    "        # gamma is a matrix of size `len(obs) x num_states`\n",
    "        # it contains the marginal probability of being in state i at time t\n",
    "\n",
    "        # xi is a tensor of size `len(obs) x num_states x num_states`\n",
    "        # it conains the marginal probability of transitioning from i to j at t\n",
    "\n",
    "        # your code here!\n",
    "        log_alpha = self.forward(obs)\n",
    "        log_beta = self.backward(obs)\n",
    "\n",
    "        # logprob = np.logaddexp(log_alpha[-1, 0], log_alpha[-1, 1])\n",
    "        # if self.num_states > 2:\n",
    "        #   for i in range(2, self.num_states):\n",
    "        #     logprob = np.logaddexp(log)\n",
    "        # print(log_alpha[-1, :])\n",
    "\n",
    "        logprob = np.log(np.sum(np.exp(log_alpha[-1, :])))\n",
    "        # print(logprob)\n",
    "        # prob1 = 0\n",
    "        # for i in range(self.num_states):\n",
    "        #   prob1 += self.pi[i] * self.B[i, obs[0]] * np.exp(log_beta[0, i])\n",
    "        # print(np.log(prob1))\n",
    "        log_xi = np.zeros((len(obs)-1, self.num_states, self.num_states))\n",
    "        gamma = np.zeros((len(obs), self.num_states))\n",
    "\n",
    "        for t in range(len(obs)-1):\n",
    "            for i in range(self.num_states):\n",
    "                for j in range(self.num_states):\n",
    "                    log_xi[t, i, j] = log_alpha[t, i] + np.log(self.A[i, j]) + np.log(self.B[j, obs[t+1]]) + log_beta[t+1, j] - logprob\n",
    "        xi = np.exp(log_xi)\n",
    "        # print(xi)\n",
    "        gamma = np.sum(xi, axis=2)\n",
    "        gamma = np.vstack((gamma, np.sum(xi[len(obs) - 2, :, :], axis=1).reshape(1, -1)))\n",
    "\n",
    "        pi_new = gamma[0, :] \n",
    "        pi_new[pi_new == 0]= sys.float_info.min\n",
    "        self.pi = pi_new / np.sum(pi_new)\n",
    "\n",
    "        return logprob, xi, gamma\n",
    "\n",
    "    def learn_unsupervised(self, corpus, num_iters):\n",
    "        \"\"\"Run the Baum Welch EM algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        for i_iter in range(num_iters):\n",
    "            # your code here\n",
    "            expected_l_si = np.zeros(self.num_states) \n",
    "            expected_l_sij = np.zeros((self.num_states, self.num_states)) \n",
    "            total_logprob = 0\n",
    "            B_new_numerator = np.zeros((self.num_states, self.num_words))\n",
    "            B_new_denomenator = np.zeros((self.num_states, self.num_words))\n",
    "            for review in corpus:\n",
    "                logprob, xi, gamma = self.forward_backward(review)\n",
    "                # your code here \n",
    "                \n",
    "                expected_sij = np.sum(xi, axis=0)\n",
    "                # print(expected_sij)\n",
    "                expected_si = np.sum(gamma[:-1, :], axis=0)\n",
    "                expected_l_si = expected_l_si + expected_si\n",
    "                expected_l_sij = expected_l_sij + expected_sij\n",
    "                total_logprob += logprob\n",
    "                # print(expected_si)\n",
    "                # print(np.sum(expected_sij, axis=1))\n",
    "\n",
    "                for j in range(self.num_states): #states\n",
    "                    for symbol in range(self.num_words): #seq\n",
    "                        indices = [idx for idx, val in enumerate(review) if val == symbol]\n",
    "                        numerator_b = sum(gamma[indices, j])\n",
    "                        denomenator_b = sum(gamma[:, j] )\n",
    "\n",
    "                        B_new_numerator[j, symbol] = numerator_b\n",
    "                        B_new_denomenator[j, symbol] = denomenator_b\n",
    "                # print(logprob)\n",
    "            print(\"log-likelihood\", total_logprob)\n",
    "            # your code here\n",
    "            # print(expected_l_sij)\n",
    "            # print(expected_l_si)\n",
    "            A_new = expected_l_sij / expected_l_si.reshape(-1, 1)\n",
    "            A_new[A_new == 0] = sys.float_info.min\n",
    "\n",
    "            B_new = np.zeros((self.num_states, self.num_words))\n",
    "            for j in range(self.num_states): #states\n",
    "                  for symbol in range(self.num_words):\n",
    "                        if B_new_denomenator[j, symbol] == 0:\n",
    "                            B_new[j, symbol] = 0\n",
    "                        else:\n",
    "                            B_new[j, symbol] = B_new_numerator[j, symbol] / B_new_denomenator[j, symbol]\n",
    "            B_new[B_new == 0] = sys.float_info.min\n",
    "            # print(B_new.shape)\n",
    "            # print(np.sum(A_new, axis=1))\n",
    "            self.A = A_new\n",
    "            self.B = B_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "log-likelihood -1457884.7228594148\n",
      "log-likelihood -2112935.295431081\n",
      "log-likelihood -2112934.401801285\n",
      "log-likelihood -2112933.551055862\n",
      "log-likelihood -2112932.9250883325\n"
     ]
    }
   ],
   "source": [
    "tokenizer = lab_util.Tokenizer()\n",
    "tokenizer.fit(train_reviews)\n",
    "train_reviews_tk = tokenizer.tokenize(train_reviews)\n",
    "print(tokenizer.vocab_size)\n",
    "# print(len(train_reviews_tk))\n",
    "hmm = HMM(num_states=50, num_words=tokenizer.vocab_size)\n",
    "hmm.learn_unsupervised(train_reviews_tk, num_iters=5)\n",
    "\n",
    "# import pickle\n",
    "# with open(\"/content/6864-hw1/hmm_r2_s10_n10.txt\", \"wb\") as fp:\n",
    "#   pickle.dump(hmm, fp, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999999999\n",
      "state 0\n",
      "than 0.03051550900008715\n",
      "amounts 0.0307816689494252\n",
      "i 0.03486354478620912\n",
      "to 0.03930483951442974\n",
      "of 0.04404783177827235\n",
      "that 0.07144674007185217\n",
      "a 0.07846087894929311\n",
      ". 0.09765691750799399\n",
      "<unk> 0.1167721775811637\n",
      ", 0.15053890651878618\n",
      "\n",
      "0.9999999999999998\n",
      "state 1\n",
      "a 0.03575000842324872\n",
      "tasted 0.03753353415943841\n",
      "drink 0.040050828760549245\n",
      "to 0.043103344784770375\n",
      "did 0.04377596283012293\n",
      "sandwiches 0.04380082757065057\n",
      ". 0.0752546233466296\n",
      "<unk> 0.0830077631985694\n",
      "it 0.0836253369521685\n",
      ", 0.1120732943854877\n",
      "\n",
      "1.0\n",
      "state 2\n",
      "drink 0.03783391162380982\n",
      "using 0.03802657757529539\n",
      "salad 0.04794739328889711\n",
      "to 0.04933921822882991\n",
      "vinegar 0.055499026776360524\n",
      ". 0.05840110460304444\n",
      "i 0.06187833756956893\n",
      "of 0.062061666971077414\n",
      "a 0.12144497272387937\n",
      ", 0.12308891869932204\n",
      "\n",
      "1.0000000000000004\n",
      "state 3\n",
      "this 0.033407287241896545\n",
      "i 0.03594345027074527\n",
      "dressing 0.053252519359956396\n",
      "so 0.053923789948647924\n",
      ". 0.055836497286819414\n",
      "but 0.06441534917399176\n",
      "it 0.06861663735427871\n",
      "than 0.08010376513662698\n",
      "was 0.11643731216990498\n",
      ", 0.15860299486684068\n",
      "\n",
      "0.9999999999999996\n",
      "state 4\n",
      "tasted 0.028169938490249076\n",
      "salad 0.03406359505315216\n",
      "this 0.03531435079520143\n",
      "amounts 0.043502803240329534\n",
      "bit 0.045915428467827965\n",
      "etc 0.048897749797267624\n",
      "i 0.05645548827280471\n",
      "but 0.07316036130704863\n",
      ", 0.14223245764841005\n",
      "<unk> 0.19952481308157122\n",
      "\n",
      "0.9999999999999997\n",
      "state 5\n",
      "etc 0.03135299423000098\n",
      "on 0.03383839764694486\n",
      "than 0.03404059138458428\n",
      "a 0.041815517081614975\n",
      "sandwiches 0.04236062703338428\n",
      "bit 0.04379598521382767\n",
      "so 0.05494646474351853\n",
      "no 0.06742227373563327\n",
      "i 0.08326509783813416\n",
      ", 0.16667051287380272\n",
      "\n",
      "1.0\n",
      "state 6\n",
      "salads 0.03526519638489942\n",
      "of 0.03965055704552546\n",
      "love 0.04402883093498751\n",
      "dressing 0.044508470424066124\n",
      "<unk> 0.05493520944795334\n",
      "tried 0.05790616648094447\n",
      "that 0.0722091999545725\n",
      "but 0.0745002446902411\n",
      "got 0.07716769951656366\n",
      ". 0.15105048958205614\n",
      "\n",
      "0.9999999999999998\n",
      "state 7\n",
      "a 0.03886378577653049\n",
      "to 0.0466540352503882\n",
      "love 0.05197977539048298\n",
      ", 0.05246563862656596\n",
      "bit 0.05433372735014634\n",
      "got 0.062402746699508224\n",
      "tried 0.07215586519759873\n",
      "but 0.08627330731804853\n",
      "it 0.09471386825369763\n",
      "i 0.10515870421825287\n",
      "\n",
      "1.0\n",
      "state 8\n",
      "to 0.03133907332171252\n",
      "of 0.035069360170654096\n",
      "did 0.03642068398508145\n",
      "really 0.045519416465565225\n",
      "etc 0.049634813580576254\n",
      "drink 0.062402992984879606\n",
      "no 0.07165068662226172\n",
      "but 0.07267840546122978\n",
      "a 0.10260737484884885\n",
      "<unk> 0.160028968409743\n",
      "\n",
      "1.0000000000000002\n",
      "state 9\n",
      "drink 0.03772801843044249\n",
      "did 0.039687816004630096\n",
      "no 0.04422634194242526\n",
      "but 0.04534510591714627\n",
      "a 0.04538842586981043\n",
      "was 0.04947265570251506\n",
      ", 0.05633676375341666\n",
      "so 0.05904958305265969\n",
      ". 0.08886101626727788\n",
      "<unk> 0.14587139120828238\n",
      "\n",
      "1.0000000000000004\n",
      "state 10\n",
      "than 0.029985341772658033\n",
      "to 0.03300472959950317\n",
      "closer 0.035624400884751266\n",
      ". 0.03791273608794449\n",
      "so 0.03806911608382129\n",
      "but 0.04082970810584918\n",
      ", 0.0586328630311657\n",
      "it 0.06733859797061324\n",
      "a 0.1393892334567839\n",
      "<unk> 0.1722586155970057\n",
      "\n",
      "0.9999999999999998\n",
      "state 11\n",
      "using 0.032798089505112545\n",
      "vinegar 0.0336181157134706\n",
      "on 0.036189345773684045\n",
      "dressing 0.0376004901255645\n",
      "amounts 0.04170446467614543\n",
      "but 0.06598903594072186\n",
      "a 0.07168732996637284\n",
      "etc 0.09561813217473815\n",
      "drink 0.10598985187288729\n",
      "it 0.10682981015450545\n",
      "\n",
      "1.0\n",
      "state 12\n",
      "i 0.031046603458855324\n",
      "but 0.03484472752829915\n",
      "did 0.041240362254441776\n",
      "closer 0.04947507189619995\n",
      "salads 0.05182726321636997\n",
      "it 0.05737444444553102\n",
      "of 0.06245071395940517\n",
      "bit 0.07752331871216606\n",
      "sandwiches 0.08692042605749092\n",
      "<unk> 0.13983883906853883\n",
      "\n",
      "0.9999999999999998\n",
      "state 13\n",
      "tried 0.03795219363003772\n",
      "but 0.0384808038339756\n",
      "drink 0.04042424415686344\n",
      "to 0.042026640238076535\n",
      "<unk> 0.06066503448208898\n",
      "like 0.06180485144251752\n",
      "bit 0.06615879474786705\n",
      "of 0.06977172154678571\n",
      ". 0.08635445502741135\n",
      "on 0.08922942660153857\n",
      "\n",
      "1.0\n",
      "state 14\n",
      "really 0.03160292706375566\n",
      "but 0.04498048711029702\n",
      "sandwiches 0.046875543106039474\n",
      "of 0.05178406729079064\n",
      "was 0.0558482732983157\n",
      ", 0.079071224904526\n",
      "<unk> 0.07967603861369144\n",
      "a 0.09204192117346298\n",
      "it 0.0927203796187862\n",
      ". 0.093873493470528\n",
      "\n",
      "1.0000000000000002\n",
      "state 15\n",
      "but 0.03486711001629042\n",
      "on 0.040096553746895226\n",
      "than 0.042308465277571615\n",
      "tasted 0.04930987127499119\n",
      "this 0.05329274928776266\n",
      "<unk> 0.06981934637396915\n",
      "a 0.07989549689963146\n",
      "to 0.08946608729278632\n",
      ", 0.10537241310616054\n",
      "it 0.14752374538413549\n",
      "\n",
      "1.0000000000000002\n",
      "state 16\n",
      "to 0.02789098276354977\n",
      "than 0.035556864378586045\n",
      ". 0.03920208010225849\n",
      "sandwiches 0.044987500209005146\n",
      "really 0.0506330623936841\n",
      "like 0.06310824878772087\n",
      ", 0.09602157068245933\n",
      "of 0.10688080935062533\n",
      "<unk> 0.12717003415372266\n",
      "using 0.16127123029250653\n",
      "\n",
      "1.0\n",
      "state 17\n",
      "sandwiches 0.002271837716057208\n",
      "was 0.002661825442478247\n",
      "of 0.004977419871806123\n",
      "<unk> 0.0052832912383679945\n",
      "a 0.00581225165340717\n",
      "it 0.007642163565373755\n",
      "drink 0.0078534982209832\n",
      "got 0.09866220769160786\n",
      "tried 0.1253788079785668\n",
      "love 0.7176553950940376\n",
      "\n",
      "1.0000000000000002\n",
      "state 18\n",
      "did 0.03595095828547607\n",
      "salads 0.036401350965504446\n",
      "really 0.03645081085948584\n",
      "sandwiches 0.04536810596003489\n",
      "of 0.05001024704551627\n",
      ", 0.0545313407916455\n",
      "i 0.06629180599042288\n",
      "it 0.06886526129665041\n",
      ". 0.07458091338640369\n",
      "<unk> 0.10915966629358449\n",
      "\n",
      "1.0000000000000002\n",
      "state 19\n",
      "<unk> 0.03059262978956018\n",
      "on 0.03176047247738827\n",
      "bit 0.03556931737125863\n",
      "it 0.037223630543299226\n",
      "to 0.03979513928506313\n",
      "a 0.04151605396238835\n",
      ". 0.04718719864413191\n",
      "got 0.06682057984238263\n",
      "tried 0.1646040599904615\n",
      "love 0.2803291663686777\n",
      "\n",
      "0.9999999999999999\n",
      "state 20\n",
      "tried 0.04104634173416952\n",
      "i 0.04396844978304443\n",
      "so 0.0450807862000616\n",
      "got 0.045479294121712296\n",
      "but 0.053806089041307396\n",
      ", 0.05457432307631623\n",
      "really 0.06004695396658951\n",
      "using 0.08279172145378408\n",
      "<unk> 0.09627222209094256\n",
      "drink 0.11590057885102281\n",
      "\n",
      "0.9999999999999998\n",
      "state 21\n",
      "salad 0.038388991324719755\n",
      "so 0.038799860042952286\n",
      "to 0.04357664993099695\n",
      "using 0.043815842056485484\n",
      ", 0.05097599634068802\n",
      "but 0.052172742252820876\n",
      "etc 0.06982170881596013\n",
      "drink 0.07334564024007441\n",
      "a 0.09033879076846028\n",
      "<unk> 0.1546758876143538\n",
      "\n",
      "1.0\n",
      "state 22\n",
      "like 0.03978478309678044\n",
      "hard 0.040976530447842704\n",
      "of 0.04185038318285942\n",
      "amounts 0.04236144554337993\n",
      "a 0.04466382613900829\n",
      ". 0.051270389076467034\n",
      "closer 0.05231215199315988\n",
      "bit 0.056419477086563745\n",
      "drink 0.08160180962376584\n",
      "to 0.106620924321602\n",
      "\n",
      "1.0000000000000004\n",
      "state 23\n",
      "etc 0.03288716182103033\n",
      "<unk> 0.03996422567259695\n",
      ". 0.0406558249027111\n",
      "on 0.04142626730672766\n",
      "i 0.04528897585844563\n",
      "drink 0.048680304755623675\n",
      "of 0.06567714669875097\n",
      "but 0.07359895313026596\n",
      ", 0.0855094894209545\n",
      "it 0.16769480573028342\n",
      "\n",
      "1.0\n",
      "state 24\n",
      "tasted 0.03832636197660999\n",
      "salads 0.043791207771769855\n",
      ". 0.04402785908459536\n",
      "etc 0.04490264357793856\n",
      "dressing 0.05445042430032699\n",
      "a 0.05519570242488315\n",
      "bit 0.05716373557450532\n",
      "drink 0.06662024804618234\n",
      "it 0.09769492557396865\n",
      "<unk> 0.1403488681794854\n",
      "\n",
      "1.0\n",
      "state 25\n",
      "but 0.03522050677153069\n",
      "i 0.04220676139948946\n",
      "go 0.043621779270643285\n",
      "a 0.048361438404200006\n",
      "so 0.051912183835085335\n",
      ", 0.0693983805646814\n",
      "it 0.07116185049757222\n",
      "<unk> 0.07501670775732354\n",
      "hard 0.08155796887428246\n",
      ". 0.15262100668908204\n",
      "\n",
      "1.0000000000000002\n",
      "state 26\n",
      "salads 0.036814808759028664\n",
      "no 0.03715336004145224\n",
      "drink 0.039375094217518336\n",
      "go 0.04768984554717754\n",
      "it 0.05368791337536237\n",
      "a 0.08468385544478371\n",
      "but 0.0947682361728965\n",
      ". 0.09937230402605884\n",
      "i 0.10125278075888891\n",
      "<unk> 0.10836360808617428\n",
      "\n",
      "1.0\n",
      "state 27\n",
      "like 0.02850629569553275\n",
      "amounts 0.03134636813761029\n",
      "really 0.03763534144910262\n",
      "to 0.044629967963659985\n",
      ". 0.05681762832546237\n",
      ", 0.06317937564960566\n",
      "of 0.07450955643970167\n",
      "a 0.09485880500686436\n",
      "it 0.10441766892047169\n",
      "<unk> 0.10637096184738105\n",
      "\n",
      "0.9999999999999999\n",
      "state 28\n",
      "salad 0.03470803486431371\n",
      "on 0.0358364808060038\n",
      "tasted 0.037778823708016385\n",
      "dressing 0.04253759172371506\n",
      "closer 0.04922817708534229\n",
      "sandwiches 0.06707899002272291\n",
      "a 0.07689069416870514\n",
      ", 0.08744962939053273\n",
      "it 0.08781802050027486\n",
      "drink 0.10377625827731801\n",
      "\n",
      "0.9999999999999997\n",
      "state 29\n",
      "using 0.036490218913407334\n",
      "go 0.0390825691448334\n",
      "i 0.040587371735838824\n",
      "closer 0.04234688558207122\n",
      "tasted 0.04593568219748385\n",
      "drink 0.053711832922672856\n",
      "of 0.06177521047836527\n",
      "a 0.10079827506818743\n",
      ". 0.10750544631728313\n",
      ", 0.14213264022216732\n",
      "\n",
      "1.0000000000000004\n",
      "state 30\n",
      "etc 0.04038588849371207\n",
      "no 0.042467307405239456\n",
      "i 0.04416606045177535\n",
      "it 0.04999214566784381\n",
      "a 0.05290375717750559\n",
      "salad 0.05314295162801231\n",
      "did 0.06050941790254558\n",
      "<unk> 0.07430058843459841\n",
      ", 0.10310540475062596\n",
      "but 0.10948433739663693\n",
      "\n",
      "1.0000000000000002\n",
      "state 31\n",
      "that 0.029766653954515183\n",
      "etc 0.02993345922186971\n",
      "this 0.04166224952228023\n",
      "got 0.04263092979187373\n",
      "to 0.047506905317297086\n",
      "was 0.05846739323607217\n",
      ". 0.07127449104684794\n",
      "a 0.10162818758684487\n",
      "<unk> 0.1050197549562068\n",
      ", 0.10914244571506246\n",
      "\n",
      "0.9999999999999998\n",
      "state 32\n",
      "to 0.032006626186440325\n",
      "than 0.03235107321342505\n",
      "got 0.03583481300478596\n",
      "vinegar 0.04956565558581383\n",
      ". 0.05773247381355608\n",
      "of 0.06997703944998958\n",
      "it 0.09050653013596481\n",
      ", 0.11111643675647567\n",
      "a 0.11943939424289969\n",
      "<unk> 0.13224005218267407\n",
      "\n",
      "1.0000000000000002\n",
      "state 33\n",
      "did 0.013049322392676354\n",
      "i 0.014189437104372074\n",
      "than 0.01752049650799158\n",
      "<unk> 0.023045953315642447\n",
      "of 0.0257251727015029\n",
      ", 0.02868986977463946\n",
      "it 0.03205885637928772\n",
      "tried 0.09055822393561247\n",
      "got 0.1972670879269056\n",
      "love 0.43007229120492596\n",
      "\n",
      "1.0000000000000002\n",
      "state 34\n",
      "sandwiches 0.0294918191321689\n",
      "did 0.032481576839664864\n",
      "salad 0.036508314958327515\n",
      ". 0.03804242063973563\n",
      "of 0.04517270444376596\n",
      "so 0.06432107131349245\n",
      "it 0.07354283389907389\n",
      "<unk> 0.07474384755505586\n",
      "drink 0.11863197304763007\n",
      ", 0.17731771063521146\n",
      "\n",
      "1.0000000000000002\n",
      "state 35\n",
      "but 0.0328950784258345\n",
      "i 0.0372552104840016\n",
      "got 0.04588065694403033\n",
      "to 0.04623949434815241\n",
      ", 0.049465698761508865\n",
      ". 0.052177344176649376\n",
      "so 0.06509020177486252\n",
      "drink 0.07605795287822234\n",
      "using 0.09086468163306868\n",
      "a 0.09509404236972634\n",
      "\n",
      "0.9999999999999998\n",
      "state 36\n",
      "closer 0.04040720956988743\n",
      "i 0.041386224835587314\n",
      "vinegar 0.042829786858034215\n",
      ". 0.04967538245931045\n",
      "a 0.06295473824332083\n",
      "to 0.06810654808248218\n",
      "but 0.07065412198387315\n",
      ", 0.08242431866888314\n",
      "it 0.09597682133178757\n",
      "<unk> 0.13398299790440168\n",
      "\n",
      "0.9999999999999999\n",
      "state 37\n",
      "to 0.0358660246756904\n",
      "of 0.037132791382365415\n",
      "it 0.039800423971197346\n",
      "sandwiches 0.0413368424395056\n",
      "tasted 0.045493900531656487\n",
      "on 0.056190269819647934\n",
      "vinegar 0.06152894321165417\n",
      ". 0.07115805328605165\n",
      "a 0.08562213739345428\n",
      "<unk> 0.10319320047711786\n",
      "\n",
      "1.0\n",
      "state 38\n",
      "to 0.041117223854779346\n",
      "i 0.04897862464685756\n",
      ". 0.051325467534790975\n",
      "so 0.05510574727497296\n",
      ", 0.05980339542171092\n",
      "but 0.06291601191579545\n",
      "a 0.06874602732196332\n",
      "drink 0.07725458158471452\n",
      "it 0.08932190200820127\n",
      "<unk> 0.14129018613157918\n",
      "\n",
      "1.0\n",
      "state 39\n",
      "of 0.00016843223157414283\n",
      "than 0.00017064864543370878\n",
      "no 0.00018017132188681228\n",
      "drink 0.000186905656849412\n",
      "it 0.0002089473397327215\n",
      ". 0.00028359449096036055\n",
      "this 0.0003432973380414122\n",
      "<unk> 0.0005777301763595387\n",
      "a 0.0007976379004407141\n",
      "i 0.9956329755264767\n",
      "\n",
      "1.0000000000000002\n",
      "state 40\n",
      "<unk> 0.019071959224204477\n",
      "drink 0.019215203135219808\n",
      "it 0.019542852045888265\n",
      "hard 0.021193972620859196\n",
      "to 0.0297565664715357\n",
      ". 0.031947905309228426\n",
      ", 0.03515782710713502\n",
      "tried 0.06897841295660935\n",
      "got 0.15435420088819551\n",
      "love 0.40006373233196196\n",
      "\n",
      "0.9999999999999999\n",
      "state 41\n",
      "got 0.0355479936595765\n",
      "closer 0.036048783073094344\n",
      "drink 0.04168163568595679\n",
      "love 0.04231051846139975\n",
      "than 0.042529030551295914\n",
      ". 0.04264443392210578\n",
      "to 0.055545242931289054\n",
      "a 0.06686074362517955\n",
      ", 0.10377837535011487\n",
      "it 0.11708838827061065\n",
      "\n",
      "1.0\n",
      "state 42\n",
      ". 0.04033787543893753\n",
      "drink 0.04427557738035211\n",
      "than 0.04731922631844834\n",
      "of 0.05218781906616568\n",
      "a 0.06574200451811416\n",
      "salad 0.066742081785842\n",
      "to 0.06683965137175855\n",
      "like 0.06864129080149431\n",
      "really 0.08224554709301626\n",
      "it 0.16587217933261283\n",
      "\n",
      "1.0\n",
      "state 43\n",
      "closer 0.029522632472347498\n",
      "hard 0.03013526110737226\n",
      "<unk> 0.036913367193646865\n",
      ". 0.04289138297085961\n",
      ", 0.05603933083959425\n",
      "drink 0.05881589822906165\n",
      "tried 0.09417277438960149\n",
      "of 0.09659998988289946\n",
      "a 0.11127390760477777\n",
      "vinegar 0.11149626656617198\n",
      "\n",
      "1.0\n",
      "state 44\n",
      "closer 0.04428721327238182\n",
      "amounts 0.04518950219616558\n",
      "drink 0.04676062512756299\n",
      "tasted 0.04679875553427405\n",
      "<unk> 0.04993386989979366\n",
      "i 0.05715257948069143\n",
      "bit 0.06585554580682887\n",
      ". 0.07267633621932647\n",
      "on 0.0900833320849609\n",
      "vinegar 0.0926497102528772\n",
      "\n",
      "1.0\n",
      "state 45\n",
      "i 0.04082958407188894\n",
      "<unk> 0.044384941731383284\n",
      "tasted 0.046832302815386306\n",
      "got 0.049812517440735435\n",
      ". 0.053920400221800106\n",
      "drink 0.058142389932005836\n",
      "bit 0.059224935105778545\n",
      ", 0.07188934188687698\n",
      "but 0.08514339457205496\n",
      "to 0.13844811234083532\n",
      "\n",
      "1.0000000000000004\n",
      "state 46\n",
      "<unk> 0.03895836332825077\n",
      "this 0.04235924758699596\n",
      "so 0.043649647460853855\n",
      "tried 0.048267651226650464\n",
      "drink 0.0504901235317118\n",
      "vinegar 0.055022405757203574\n",
      "to 0.07517709938254061\n",
      "a 0.0923302034109614\n",
      ", 0.09670833276553473\n",
      "but 0.11894647401304591\n",
      "\n",
      "1.0000000000000002\n",
      "state 47\n",
      "amounts 0.03132644623457437\n",
      "sandwiches 0.03155665596608034\n",
      "no 0.03535476938106048\n",
      "drink 0.035865058781499086\n",
      "of 0.05140685297392127\n",
      "to 0.06134500839081872\n",
      "that 0.07808699415642296\n",
      ", 0.07838169411117722\n",
      "it 0.09473830370308726\n",
      ". 0.12783299395150438\n",
      "\n",
      "1.0\n",
      "state 48\n",
      "it 0.03877698461270572\n",
      "this 0.04160447056604041\n",
      "no 0.044431070037562186\n",
      "but 0.046168933840339876\n",
      ". 0.05293371335043537\n",
      "really 0.06163267803040507\n",
      "drink 0.06516605211642584\n",
      ", 0.06554142714005065\n",
      "to 0.07651387004852273\n",
      "<unk> 0.16007813580035396\n",
      "\n",
      "0.9999999999999998\n",
      "state 49\n",
      "dressing 0.0349384233624517\n",
      "drink 0.037365743166526674\n",
      "did 0.04387461687514161\n",
      ", 0.048096519157278576\n",
      ". 0.05330087343887381\n",
      "it 0.055170496921718604\n",
      "than 0.07069850422372809\n",
      "etc 0.07362177569382372\n",
      "so 0.07977190770500621\n",
      "a 0.1374653556427367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(hmm.num_states):\n",
    "    print(np.sum(hmm.B[i, :]))\n",
    "    most_probable = np.argsort(hmm.B[i, :])[-10:]\n",
    "    print(f\"state {i}\")\n",
    "    for o in most_probable:\n",
    "        print(tokenizer.token_to_word[o], hmm.B[i, o])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so closer a really this using sandwiches so did ,']\n",
      "['love . <unk> got to a it on this no']\n",
      "['sandwiches , to <unk> but bit , to it it']\n",
      "['i a was on using amounts amounts it salad i']\n",
      "['i . , vinegar of of but but to i']\n",
      "['i love <unk> <unk> . amounts . like a on']\n",
      "['vinegar hard but did was a , sandwiches <unk> i']\n",
      "['tasted , but it . using like so really vinegar']\n",
      "['i i than go but hard a closer love but']\n",
      "['go . tasted this sandwiches got did on to to']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokenizer.de_tokenize([hmm.generate(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hmm features, 3000 examples\n",
      "test accuracy 0.544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_model(xs_featurized, ys):\n",
    "    import sklearn.linear_model\n",
    "    model = sklearn.linear_model.LogisticRegression()\n",
    "    model.fit(xs_featurized, ys)\n",
    "    return model\n",
    "\n",
    "def eval_model(model, xs_featurized, ys):\n",
    "    pred_ys = model.predict(xs_featurized)\n",
    "    print(\"test accuracy\", np.mean(pred_ys == ys))\n",
    "\n",
    "def training_experiment(name, featurizer, n_train):\n",
    "    print(f\"{name} features, {n_train} examples\")\n",
    "#     hmm_featurizer(tokenizer.tokenize([train_reviews[:n_train][0]]))\n",
    "#     print(tokenizer.tokenize([train_reviews[:n_train][0]]))\n",
    "    train_xs = np.array([\n",
    "        hmm_featurizer(tokenizer.tokenize([review])) \n",
    "        for review in train_reviews[:n_train]\n",
    "    ])\n",
    "    train_ys = train_labels[:n_train]\n",
    "    test_xs = np.array([\n",
    "        hmm_featurizer(tokenizer.tokenize([review]))\n",
    "        for review in test_reviews\n",
    "    ])\n",
    "    test_ys = test_labels\n",
    "    model = train_model(train_xs, train_ys)\n",
    "    eval_model(model, test_xs, test_ys)\n",
    "    print()\n",
    "\n",
    "def hmm_featurizer(review):\n",
    "    _, _, gamma = hmm.forward_backward(review[0])\n",
    "    return gamma.sum(axis=0)\n",
    "\n",
    "training_experiment(\"hmm\", hmm_featurizer, n_train=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./hmm_results/hmm_r_s50_n5.txt\", \"wb\") as fp:\n",
    "    pickle.dump(hmm, fp, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
