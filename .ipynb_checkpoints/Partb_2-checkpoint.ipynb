{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import lab_util\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = []\n",
    "n_positive = 0\n",
    "n_disp = 0\n",
    "with open(\"./reviews.csv\") as reader:\n",
    "    csvreader = csv.reader(reader)\n",
    "    next(csvreader)\n",
    "    for id, review, label in csvreader:\n",
    "        label = int(label)\n",
    "        # hacky class balancing\n",
    "        if label == 1:\n",
    "            if n_positive == 2000:\n",
    "                continue\n",
    "            n_positive += 1\n",
    "        if len(data) == 4000:\n",
    "            break\n",
    "        data.append((review, label))\n",
    "\n",
    "        if n_disp > 5:\n",
    "            continue\n",
    "        n_disp += 1\n",
    "        # print(\"review:\", review)\n",
    "        # print(\"rating:\", label, \"(good)\" if label == 1 else \"(bad)\")\n",
    "        # print()\n",
    "\n",
    "# print(f\"Read {len(data)} total reviews.\")\n",
    "np.random.shuffle(data)\n",
    "reviews, labels = zip(*data)\n",
    "train_reviews = reviews[:3000]\n",
    "train_labels = labels[:3000]\n",
    "val_reviews = reviews[3000:3500]\n",
    "val_labels = labels[3000:3500]\n",
    "test_reviews = reviews[3500:]\n",
    "test_labels = labels[3500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm model\n",
    "class HMM(object):\n",
    "    def __init__(self, num_states, num_words):\n",
    "        self.num_states = num_states\n",
    "        self.num_words = num_words\n",
    "\n",
    "        self.states = range(num_states)\n",
    "        self.symbols = range(num_words)\n",
    "\n",
    "        # initialize the matrix A with random transition probabilities p(j|i)\n",
    "        # A should be a matrix of size `num_states x num_states`\n",
    "        # with rows that sum to 1\n",
    "        self.A = np.random.random([self.num_states, self.num_states])\n",
    "        self.A = self.A / np.sum(self.A, axis=1).reshape(-1, 1) # your code here\n",
    "\n",
    "        # initialize the matrix B with random emission probabilities p(o|i)\n",
    "        # B should be a matrix of size `num_states x num_words`\n",
    "        # with rows that sum to 1\n",
    "        self.B = np.random.random([self.num_states, self.num_words])\n",
    "        self.B = self.B / np.sum(self.B, axis=1).reshape(-1, 1)\n",
    "        # your code here\n",
    "\n",
    "        # initialize the vector pi with a random starting distribution\n",
    "        # pi should be a vector of size `num_states`\n",
    "        self.pi = np.random.random(self.num_states)\n",
    "        self.pi = self.pi / np.sum(self.pi) \n",
    "        # your code here\n",
    "\n",
    "    def generate(self, n):\n",
    "        \"\"\"randomly sample the HMM to generate a sequence.\n",
    "        \"\"\"\n",
    "        # we'll give you this one\n",
    "\n",
    "        sequence = []\n",
    "        # initialize the first state\n",
    "        state = np.random.choice(self.states, p=self.pi)\n",
    "        for i in range(n):\n",
    "            # get the emission probs for this state\n",
    "            b = self.B[state, :]\n",
    "            # emit a word\n",
    "            word = np.random.choice(self.symbols, p=b)\n",
    "            sequence.append(word)\n",
    "            # get the transition probs for this state\n",
    "            a = self.A[state, :]\n",
    "            # update the state\n",
    "            state = np.random.choice(self.states, p=a)\n",
    "        return sequence\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # run the forward algorithm\n",
    "        # this function should return a `len(obs) x num_states` matrix\n",
    "        # where the (i, j)th entry contains p(obs[:t], hidden_state_t = i)\n",
    "\n",
    "        alpha = np.zeros((len(obs), self.num_states))\n",
    "        # your code here!\n",
    "        # print(self.pi)\n",
    "        alpha[0, :] = self.pi * self.B[:, obs[0]]\n",
    "        # print(alphal[0,:])\n",
    "        for t in range(1, len(obs)):\n",
    "            for j in range(self.num_states):\n",
    "            # alpha[t, j] = np.dot(alpha[t-1, :], self.A[:, j]) * self.B[j, obs[t]]\n",
    "                alpha[t, j] = alpha[t-1].dot(self.A[:, j]) * self.B[j, obs[t]]\n",
    "        alpha[alpha == 0] = sys.float_info.min\n",
    "        alpha = np.log(alpha)\n",
    "\n",
    "        return alpha\n",
    "\n",
    "    def backward(self, obs):\n",
    "        # run the backward algorithm\n",
    "        # this function should return a `len(obs) x num_states` matrix\n",
    "        # where the (i, j)th entry contains p(obs[t+1:] | hidden_state_t = i)\n",
    "\n",
    "        beta = np.zeros((len(obs), self.num_states))\n",
    "        # your code here!\n",
    "        beta[len(obs) - 1, :] = np.ones(self.num_states)\n",
    "        for t in range(len(obs)-2, -1, -1):\n",
    "            for j in range(self.num_states):\n",
    "                beta[t, j] = (beta[t+1, :] * self.B[:, obs[t+1]]).dot(self.A[j, :])\n",
    "\n",
    "        beta[beta == 0] = sys.float_info.min\n",
    "        beta = np.log(beta)\n",
    "\n",
    "        return beta\n",
    "        \n",
    "    def forward_backward(self, obs):\n",
    "        # compute forward--backward scores\n",
    "\n",
    "        # logprob is the total log-probability of the sequence obs \n",
    "        # (marginalizing over hidden states)\n",
    "\n",
    "        # gamma is a matrix of size `len(obs) x num_states`\n",
    "        # it contains the marginal probability of being in state i at time t\n",
    "\n",
    "        # xi is a tensor of size `len(obs) x num_states x num_states`\n",
    "        # it conains the marginal probability of transitioning from i to j at t\n",
    "\n",
    "        # your code here!\n",
    "        log_alpha = self.forward(obs)\n",
    "        log_beta = self.backward(obs)\n",
    "\n",
    "        # logprob = np.logaddexp(log_alpha[-1, 0], log_alpha[-1, 1])\n",
    "        # if self.num_states > 2:\n",
    "        #   for i in range(2, self.num_states):\n",
    "        #     logprob = np.logaddexp(log)\n",
    "        # print(log_alpha[-1, :])\n",
    "\n",
    "        logprob = np.log(np.sum(np.exp(log_alpha[-1, :])))\n",
    "        # print(logprob)\n",
    "        # prob1 = 0\n",
    "        # for i in range(self.num_states):\n",
    "        #   prob1 += self.pi[i] * self.B[i, obs[0]] * np.exp(log_beta[0, i])\n",
    "        # print(np.log(prob1))\n",
    "        log_xi = np.zeros((len(obs)-1, self.num_states, self.num_states))\n",
    "        gamma = np.zeros((len(obs), self.num_states))\n",
    "\n",
    "        for t in range(len(obs)-1):\n",
    "            for i in range(self.num_states):\n",
    "                for j in range(self.num_states):\n",
    "                    log_xi[t, i, j] = log_alpha[t, i] + np.log(self.A[i, j]) + np.log(self.B[j, obs[t+1]]) + log_beta[t+1, j] - logprob\n",
    "        xi = np.exp(log_xi)\n",
    "        # print(xi)\n",
    "        gamma = np.sum(xi, axis=2)\n",
    "        gamma = np.vstack((gamma, np.sum(xi[len(obs) - 2, :, :], axis=1).reshape(1, -1)))\n",
    "\n",
    "        pi_new = gamma[0, :] \n",
    "        pi_new[pi_new == 0]= sys.float_info.min\n",
    "        self.pi = pi_new / np.sum(pi_new)\n",
    "\n",
    "        return logprob, xi, gamma\n",
    "\n",
    "    def learn_unsupervised(self, corpus, num_iters):\n",
    "        \"\"\"Run the Baum Welch EM algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        for i_iter in range(num_iters):\n",
    "            # your code here\n",
    "            expected_l_si = np.zeros(self.num_states) \n",
    "            expected_l_sij = np.zeros((self.num_states, self.num_states)) \n",
    "            total_logprob = 0\n",
    "            B_new_numerator = np.zeros((self.num_states, self.num_words))\n",
    "            B_new_denomenator = np.zeros((self.num_states, self.num_words))\n",
    "            for review in corpus:\n",
    "                logprob, xi, gamma = self.forward_backward(review)\n",
    "                # your code here \n",
    "                \n",
    "                expected_sij = np.sum(xi, axis=0)\n",
    "                # print(expected_sij)\n",
    "                expected_si = np.sum(gamma[:-1, :], axis=0)\n",
    "                expected_l_si = expected_l_si + expected_si\n",
    "                expected_l_sij = expected_l_sij + expected_sij\n",
    "                total_logprob += logprob\n",
    "                # print(expected_si)\n",
    "                # print(np.sum(expected_sij, axis=1))\n",
    "\n",
    "                for j in range(self.num_states): #states\n",
    "                    for symbol in range(self.num_words): #seq\n",
    "                        indices = [idx for idx, val in enumerate(review) if val == symbol]\n",
    "                        numerator_b = sum(gamma[indices, j])\n",
    "                        denomenator_b = sum(gamma[:, j] )\n",
    "\n",
    "                    B_new_numerator[j, symbol] = numerator_b\n",
    "                    B_new_denomenator[j, symbol] = denomenator_b\n",
    "                # print(logprob)\n",
    "            print(\"log-likelihood\", total_logprob)\n",
    "            # your code here\n",
    "            # print(expected_l_sij)\n",
    "            # print(expected_l_si)\n",
    "            A_new = expected_l_sij / expected_l_si.reshape(-1, 1)\n",
    "            A_new[A_new == 0] = sys.float_info.min\n",
    "\n",
    "            B_new = np.zeros((self.num_states, self.num_words))\n",
    "            for j in range(self.num_states): #states\n",
    "                  for symbol in range(self.num_words):\n",
    "                    if B_new_denomenator[j, symbol] == 0:\n",
    "                        B_new[j, symbol] = 0\n",
    "                    else:\n",
    "                        B_new[j, symbol] = B_new_numerator[j, symbol] / B_new_denomenator[j, symbol]\n",
    "            B_new[B_new == 0] = sys.float_info.min\n",
    "            # print(B_new.shape)\n",
    "            # print(np.sum(A_new, axis=1))\n",
    "            self.A = A_new\n",
    "            self.B = B_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n"
     ]
    }
   ],
   "source": [
    "tokenizer = lab_util.Tokenizer()\n",
    "tokenizer.fit(train_reviews)\n",
    "train_reviews_tk = tokenizer.tokenize(train_reviews)\n",
    "print(tokenizer.vocab_size)\n",
    "# print(len(train_reviews_tk))\n",
    "hmm = HMM(num_states=100, num_words=tokenizer.vocab_size)\n",
    "hmm.learn_unsupervised(train_reviews_tk, num_iters=10)\n",
    "\n",
    "# import pickle\n",
    "# with open(\"/content/6864-hw1/hmm_r2_s10_n10.txt\", \"wb\") as fp:\n",
    "#   pickle.dump(hmm, fp, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
